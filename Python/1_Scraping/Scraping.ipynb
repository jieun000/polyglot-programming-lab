{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c56dab24",
   "metadata": {},
   "source": [
    "ⓒWeb Scraping with Python / 라이언 미첼 저"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c2c9d6",
   "metadata": {},
   "source": [
    "#### Web Scraping(웹 스크레이핑): 데이터를 수집하는 작업 전체\n",
    "<sup>API를 활용하는 프로그램이나 사람이 직접 웹 브라우저를 조작하는 방법 제외</sup>\n",
    "- 데이터 분석, 자연어 처리, 정보 보안 등 다양한 프로그래밍 테크닉과 기술을 포괄.\n",
    "- 프로그램을 만들어 웹 서버에 쿼리를 보내 데이터(HTML 등)를 요청하고, 이를 파싱해 필요한 정보를 추천하는 작업을 자동으로 함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89e1fd9",
   "metadata": {},
   "source": [
    "[urllib 라이브러리](https://docs.python.org/3/library/urllib.html): 웹을 통해 데이터를 요청하는 함수, 쿠키를 처리하는 함수, 헤저나 유저 에이전트 등 메타데이터를 바꾸는 함수 등.\n",
    "- urlopen: 네트워크를 통해 원격 객체를 읽음. html파일이나 이미지 파일, 기타 파일 스트림을 열 수 있는 매우 범용적인 함수."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef4a923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "html = urlopen('http://python.cyber.co.kr/pds/books/python2nd/test2.html')\n",
    "print(html.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364fc6dc",
   "metadata": {},
   "source": [
    "#### BeautifulSoup: 잘못된 HTML을 수정하여 XML 형식의 파이썬 객체로 변환\n",
    "아름다운 수프, 풍부한 녹색 <br>\n",
    "그릇에서 기다리거라! <br>\n",
    "누가 이 맛있는 것에 숙이지 않으리? <br>\n",
    "저녁 수프, 아름다운 수프! <br>\n",
    "    - Alice in Wonderland\n",
    "    \n",
    "BeautifulSoup(객체의 근간인 HTML 텍스트, 구문 분석기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0e140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('http://python.cyber.co.kr/pds/books/python2nd/test2.html')\n",
    "# bs = BeautifulSoup(html.read(), \"html.parser\")\n",
    "bs = BeautifulSoup(html, \"html.parser\")\n",
    "print(bs)\n",
    "print(bs.h1) # bs.html.body.h1 || bs.body.h1 || bs.html.h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b017c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lxml: 닫히지 않은 태그, 계층 구조가 잘못된 태그, <head><body>태그가 없는 등의 문제를 수정해줌.\n",
    "! pip install lxml\n",
    "bs = BeautifulSoup(html.read(), 'lxml')\n",
    "print(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659f2d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# html5lib: 더 많은 문제를 해결해주나 느림.\n",
    "! pip install html5lib\n",
    "bs = BeautifulSoup(html.read(), 'html5lib')\n",
    "print(bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3548dae",
   "metadata": {},
   "source": [
    "- 페이지를 찾을 수 없거나, URL 해석에 에러가 생긴 경우 <br>\n",
    "\"404 Page Not Found\", \"500 Internal Server Error\" => HTTPError\n",
    "- 서버를 찾을 수 없는 경우 <br>\n",
    "=> None 객체를 반환해 AttributeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59814587",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def getTitle(url):\n",
    "    try:\n",
    "        html = urlopen(url)\n",
    "    except HTTPError as e:\n",
    "        return None\n",
    "    try:\n",
    "        bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "        title = bs.body.h2\n",
    "    except AttributeError as e:\n",
    "        return None\n",
    "    return title\n",
    "\n",
    "title = getTitle('http://python.cyber.co.kr/pds/books/python2nd/test2.html')\n",
    "if title == None:\n",
    "    print('Title could not be found')\n",
    "else: \n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5993dcf0",
   "metadata": {},
   "source": [
    "findAll(tagName, tagAttributes)\n",
    "\n",
    "- findAll(tag, attributes, recursive, text, limit, keywords)\n",
    "- find(tag, attributes, recursive, text, keywords)\n",
    "\n",
    "recursive은 True or False(최상위 태그만 찾음) <br>\n",
    "ex) <br>\n",
    "bs.findAll({'h1', 'h2', 'h3', 'h4', 'h5'})\n",
    "bs.findAll('span', {'class': {'green', 'red'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3f5466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html= urlopen('http://www.pythonscraping.com/pages/warandpeace.html')\n",
    "bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "nameList = bs.findAll('span', {'class':'green'})\n",
    "for name in nameList:\n",
    "    print(name.get_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b8460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nameList = bs.findAll(text = 'the prince')\n",
    "print(len(nameList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a298df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html= urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
    "bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "\n",
    "print('자식과 자손')\n",
    "for child in bs.find('table', {'id': 'giftList'}).children:\n",
    "    print(child)\n",
    "\n",
    "print('형제 다루기')\n",
    "for sibling in bs.find('table', {'id': 'giftList'}).tr.next_siblings:\n",
    "    print(sibling)\n",
    "    \n",
    "print('부모 다루기:', bs.find('img', {'src': '../img/gifts/img1.jpg'}).parent.previous_sibling.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ed4b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "html= urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
    "bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "images = bs.findAll('img', {'src': re.compile('\\.\\.\\/img\\/gifts/img.*\\.jpg')}) # ../img/gifts/img로 시작해서 .jpg로 끝남\n",
    "for image in images:\n",
    "    print(image['src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 속성이 두 개인 태그 모두 추출\n",
    "bs.findAll(lambda tag: len(tag.attrs) == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278cd8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs.findAll(lambda tag: tag.get_text() == 'Or maybe he\\'s only resting?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a3cbf9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5ede96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의의 위키백과 페이지를 가져와 페이지에 들어있는 링크 목록을 가져옴\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "html = urlopen('http://en.wikipedia.org/wiki/Kevin_Bacon')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "for link in bs.find_all('a'):\n",
    "    if 'href' in link.attrs:\n",
    "        print(link.attrs['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620c7fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규 표현식 사용 => 항목 페이지를 가리키는 링크만 가져옴.\n",
    "from urllib.request import urlopen \n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "\n",
    "html = urlopen('http://en.wikipedia.org/wiki/Kevin_Bacon')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "for link in bs.find('div', {'id':'bodyContent'}).find_all(\n",
    "    'a', href=re.compile('^(/wiki/)((?!:).)*$')):\n",
    "    if 'href' in link.attrs:\n",
    "        print(link.attrs['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7460665e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import random\n",
    "import re\n",
    "\n",
    "random.seed(datetime.datetime.now().timestamp())\n",
    "\n",
    "# /wiki/<article_name>형태의 위키백과 항목 url을 받아, 링크된 항목 url 목록 전체를 반환하는 함수\n",
    "def getLinks(articleUrl):\n",
    "    html = urlopen('http://en.wikipedia.org{}'.format(articleUrl))\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    return bs.find('div', {'id':'bodyContent'}).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$'))\n",
    "\n",
    "links = getLinks('/wiki/Kevin_Bacon')\n",
    "while len(links) > 0:\n",
    "    newArticle = links[random.randint(0, len(links)-1)].attrs['href']\n",
    "    print(newArticle)\n",
    "    links = getLinks(newArticle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f278ae5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 사이트 크롤링, 중복 크롤링 없이.\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "pages = set() # set: 순서 X, 중복X\n",
    "def getLinks(pageUrl):\n",
    "    global pages\n",
    "    html = urlopen('http://en.wikipedia.org{}'.format(pageUrl))\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    try:\n",
    "        print(bs.h1.get_text())\n",
    "        print(bs.find(id ='mw-content-text').find_all('p')[0])\n",
    "        print(bs.find(id='ca-edit').find('span').find('a').attrs['href'])\n",
    "    except AttributeError:\n",
    "        print('This page is missing something! Continuing.')\n",
    "    \n",
    "    for link in bs.find_all('a', href=re.compile('^(/wiki/)')):\n",
    "        if 'href' in link.attrs:\n",
    "            if link.attrs['href'] not in pages:\n",
    "                #We have encountered a new page\n",
    "                newPage = link.attrs['href']\n",
    "                print('-'*20)\n",
    "                print(newPage)\n",
    "                pages.add(newPage)\n",
    "                getLinks(newPage)\n",
    "getLinks('') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64c3485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인터넷 크롤링, http://oreilly.com에서 시작해 외부 링크에서 외부 링크로 무작위 이동.\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "pages = set()\n",
    "random.seed(datetime.datetime.now().timestamp())\n",
    "\n",
    "#Retrieves a list of all Internal links found on a page\n",
    "# 페이지에서 발견된 내부 링크를 모두 목록으로 만듦.\n",
    "def getInternalLinks(bs, includeUrl):\n",
    "    includeUrl = '{}://{}'.format(urlparse(includeUrl).scheme, urlparse(includeUrl).netloc)\n",
    "    internalLinks = []\n",
    "    #Finds all links that begin with a \"/\", /로 시작하는 링크를 모두 찾음\n",
    "    for link in bs.find_all('a', href=re.compile('^(/|.*'+includeUrl+')')):\n",
    "        if link.attrs['href'] is not None:\n",
    "            if link.attrs['href'] not in internalLinks:\n",
    "                if(link.attrs['href'].startswith('/')):\n",
    "                    internalLinks.append(includeUrl+link.attrs['href'])\n",
    "                else:\n",
    "                    internalLinks.append(link.attrs['href'])\n",
    "    return internalLinks\n",
    "            \n",
    "#Retrieves a list of all external links found on a page\n",
    "# 페이지에서 발견된 외부 링크를 모두 목록으로 만듦\n",
    "def getExternalLinks(bs, excludeUrl):\n",
    "    externalLinks = []\n",
    "    #Finds all links that start with \"http\" that do\n",
    "    #not contain the current URL\n",
    "    # 현재 url을 포함하지 않으면서 http나 www로 시작하는 링크를 모두 찾음.\n",
    "    for link in bs.find_all('a', href=re.compile('^(http|www)((?!'+excludeUrl+').)*$')):\n",
    "        if link.attrs['href'] is not None:\n",
    "            if link.attrs['href'] not in externalLinks:\n",
    "                externalLinks.append(link.attrs['href'])\n",
    "    return externalLinks\n",
    "\n",
    "def getRandomExternalLink(startingPage):\n",
    "    html = urlopen(startingPage)\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    externalLinks = getExternalLinks(bs, urlparse(startingPage).netloc)\n",
    "    if len(externalLinks) == 0:\n",
    "        print('No external links, looking around the site for one')\n",
    "        domain = '{}://{}'.format(urlparse(startingPage).scheme, urlparse(startingPage).netloc)\n",
    "        internalLinks = getInternalLinks(bs, domain)\n",
    "        return getRandomExternalLink(internalLinks[random.randint(0,\n",
    "                                    len(internalLinks)-1)])\n",
    "    else:\n",
    "        return externalLinks[random.randint(0, len(externalLinks)-1)]\n",
    "    \n",
    "def followExternalOnly(startingSite):\n",
    "    externalLink = getRandomExternalLink(startingSite)\n",
    "    print('Random external link is: {}'.format(externalLink))\n",
    "    followExternalOnly(externalLink)\n",
    "            \n",
    "followExternalOnly('http://oreilly.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bce9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collects a list of all external URLs found on the site\n",
    "# 사이트에서 찾은 외부 URL을 모두 리스트로 수집함.\n",
    "allExtLinks = set()\n",
    "allIntLinks = set()\n",
    "\n",
    "def getAllExternalLinks(siteUrl):\n",
    "    html = urlopen(siteUrl)\n",
    "    domain = '{}://{}'.format(urlparse(siteUrl).scheme,\n",
    "                              urlparse(siteUrl).netloc)\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    internalLinks = getInternalLinks(bs, domain)\n",
    "    externalLinks = getExternalLinks(bs, domain)\n",
    "\n",
    "    for link in externalLinks:\n",
    "        if link not in allExtLinks:\n",
    "            allExtLinks.add(link)\n",
    "            print(link)\n",
    "    for link in internalLinks:\n",
    "        if link not in allIntLinks:\n",
    "            allIntLinks.add(link)\n",
    "            getAllExternalLinks(link)\n",
    "\n",
    "allIntLinks.add('http://oreilly.com')\n",
    "getAllExternalLinks('http://oreilly.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97c850d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1748a769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이아웃 다루기\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "class Content:\n",
    "    def __init__(self, url, title, body):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "\n",
    "\n",
    "def getPage(url):\n",
    "    req = requests.get(url)\n",
    "    return BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "\n",
    "def scrapeNYTimes(url):\n",
    "    bs = getPage(url)\n",
    "    title = bs.find('h1').text\n",
    "    lines = bs.select('div.StoryBodyCompanionColumn div p')\n",
    "    body = '\\n'.join([line.text for line in lines])\n",
    "    return Content(url, title, body)\n",
    "\n",
    "def scrapeBrookings(url):\n",
    "    bs = getPage(url)\n",
    "    title = bs.find('h1').text\n",
    "    body = bs.find('div', {'class', 'post-body'}).text\n",
    "    return Content(url, title, body)\n",
    "\n",
    "\n",
    "url = 'https://www.brookings.edu/blog/future-development/2018/01/26/delivering-inclusive-urban-access-3-uncomfortable-truths/'\n",
    "content = scrapeBrookings(url)\n",
    "print('Title: {}'.format(content.title))\n",
    "print('URL: {}\\n'.format(content.url))\n",
    "print(content.body)\n",
    "\n",
    "url = 'https://www.nytimes.com/2018/01/25/opinion/sunday/silicon-valley-immortality.html'\n",
    "content = scrapeNYTimes(url)\n",
    "print('Title: {}'.format(content.title))\n",
    "print('URL: {}\\n'.format(content.url))\n",
    "print(content.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489e2ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색을 통한 사이트 크롤링\n",
    "\n",
    "class Content:\n",
    "    \"\"\"Common base class for all articles/pages\"\"\"\n",
    "    \"\"\"글/페이지 전체에 사용할 기반 클래스\"\"\"\n",
    "\n",
    "    def __init__(self, topic, url, title, body):\n",
    "        self.topic = topic\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "        self.url = url\n",
    "\n",
    "    def print(self):\n",
    "        \"\"\"Flexible printing function controls output\"\"\"\n",
    "        \"\"\"출력 결과를 원하는 대로 바꿀 수 있는 함수\"\"\"\n",
    "\n",
    "        print('New article found for topic: {}'.format(self.topic))\n",
    "        print('URL: {}'.format(self.url))\n",
    "        print('TITLE: {}'.format(self.title))\n",
    "        print('BODY:\\n{}'.format(self.body))\n",
    "        \n",
    "class Website:\n",
    "    \"\"\"Contains information about website structure\"\"\"\n",
    "    \"\"\"웹사이트 구조에 관한 정보를 저장할 클래스\"\"\"\n",
    "    def __init__(self, name, url, searchUrl, resultListing, resultUrl, absoluteUrl, titleTag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.searchUrl = searchUrl\n",
    "        self.resultListing = resultListing\n",
    "        self.resultUrl = resultUrl\n",
    "        self.absoluteUrl = absoluteUrl\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag\n",
    "        \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class Crawler:\n",
    "\n",
    "    def getPage(self, url):\n",
    "        try:\n",
    "            req = requests.get(url)\n",
    "        except requests.exceptions.RequestException:\n",
    "            return None\n",
    "        return BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "    def safeGet(self, pageObj, selector):\n",
    "        childObj = pageObj.select(selector)\n",
    "        if childObj is not None and len(childObj) > 0:\n",
    "            return childObj[0].get_text()\n",
    "        return ''\n",
    "\n",
    "    def search(self, topic, site):\n",
    "        \"\"\"Searches a given website for a given topic and records all pages found\"\"\"\n",
    "        \"\"\"주어진 검색어로 주어진 웹사이트를 검색해 결과 페이지를 모두 기록\"\"\"\n",
    "        bs = self.getPage(site.searchUrl + topic)\n",
    "        searchResults = bs.select(site.resultListing)\n",
    "        for result in searchResults:\n",
    "            url = result.select(site.resultUrl)[0].attrs['href']\n",
    "            # Check to see whether it's a relative or an absolute URL\n",
    "            # 상대 URL인지 절대 URL인지 확인\n",
    "            if(site.absoluteUrl):\n",
    "                bs = self.getPage(url)\n",
    "            else:\n",
    "                bs = self.getPage(site.url + url)\n",
    "            if bs is None:\n",
    "                print('Something was wrong with that page or URL. Skipping!')\n",
    "                return\n",
    "            title = self.safeGet(bs, site.titleTag)\n",
    "            body = self.safeGet(bs, site.bodyTag)\n",
    "            if title != '' and body != '':\n",
    "                content = Content(topic, title, body, url)\n",
    "                content.print()\n",
    "\n",
    "\n",
    "crawler = Crawler()\n",
    "\n",
    "siteData = [\n",
    "    ['O\\'Reilly Media', 'http://oreilly.com', 'https://ssearch.oreilly.com/?q=',\n",
    "        'article.product-result', 'p.title a', True, 'h1', 'section#product-description'],\n",
    "    ['Reuters', 'http://reuters.com', 'http://www.reuters.com/search/news?blob=', 'div.search-result-content',\n",
    "        'h3.search-result-title a', False, 'h1', 'div.StandardArticleBody_body_1gnLA'],\n",
    "    ['Brookings', 'http://www.brookings.edu', 'https://www.brookings.edu/search/?s=',\n",
    "        'div.list-content article', 'h4.title a', True, 'h1', 'div.post-body']\n",
    "]\n",
    "sites = []\n",
    "for row in siteData:\n",
    "    sites.append(Website(row[0], row[1], row[2],\n",
    "                         row[3], row[4], row[5], row[6], row[7]))\n",
    "\n",
    "topics = ['python', 'data science']\n",
    "for topic in topics:\n",
    "    print('GETTING INFO ABOUT: ' + topic)\n",
    "    for targetSite in sites:\n",
    "        crawler.search(topic, targetSite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547be373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 링크를 통헌 사이트 크롤링\n",
    "class Website:\n",
    "\n",
    "    def __init__(self, name, url, targetPattern, absoluteUrl, titleTag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.targetPattern = targetPattern\n",
    "        self.absoluteUrl = absoluteUrl\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag\n",
    "\n",
    "\n",
    "class Content:\n",
    "\n",
    "    def __init__(self, url, title, body):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "\n",
    "    def print(self):\n",
    "        print('URL: {}'.format(self.url))\n",
    "        print('TITLE: {}'.format(self.title))\n",
    "        print('BODY:\\n{}'.format(self.body))\n",
    "        \n",
    "import re\n",
    "\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self, site):\n",
    "        self.site = site\n",
    "        self.visited = []\n",
    "\n",
    "    def getPage(self, url):\n",
    "        try:\n",
    "            req = requests.get(url)\n",
    "        except requests.exceptions.RequestException:\n",
    "            return None\n",
    "        return BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "    def safeGet(self, pageObj, selector):\n",
    "        selectedElems = pageObj.select(selector)\n",
    "        if selectedElems is not None and len(selectedElems) > 0:\n",
    "            return '\\n'.join([elem.get_text() for elem in selectedElems])\n",
    "        return ''\n",
    "\n",
    "    def parse(self, url):\n",
    "        bs = self.getPage(url)\n",
    "        if bs is not None:\n",
    "            title = self.safeGet(bs, self.site.titleTag)\n",
    "            body = self.safeGet(bs, self.site.bodyTag)\n",
    "            if title != '' and body != '':\n",
    "                content = Content(url, title, body)\n",
    "                content.print()\n",
    "\n",
    "    def crawl(self):\n",
    "        \"\"\"Get pages from website home page\"\"\"\n",
    "        \"\"\"사이트 홈페이지에서 페이지를 가져옴\"\"\"\n",
    "        bs = self.getPage(self.site.url)\n",
    "        targetPages = bs.findAll('a', href=re.compile(self.site.targetPattern))\n",
    "        for targetPage in targetPages:\n",
    "            targetPage = targetPage.attrs['href']\n",
    "            if targetPage not in self.visited:\n",
    "                self.visited.append(targetPage)\n",
    "                if not self.site.absoluteUrl:\n",
    "                    targetPage = '{}{}'.format(self.site.url, targetPage)\n",
    "                self.parse(targetPage)\n",
    "\n",
    "\n",
    "reuters = Website('Reuters', #Website.name\n",
    "                  'https://www.reuters.com', #Website.url\n",
    "                  '^(/article/)', #Website.targetPattern\n",
    "                  False, #Website.absoluteUrl\n",
    "                  'h1', #Website..titleTag\n",
    "                  'div.StandardArticleBody_body_1gnLA') #Website.bodyTag\n",
    "crawler = Crawler(reuters)\n",
    "crawler.crawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5860fb1f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2f4879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원격 url 파일 내려받기\n",
    "from urllib.request import urlretrieve\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "imageLocation = bs.find('a', {'id': 'logo'}).find('img')['src']\n",
    "urlretrieve (imageLocation, 'logo.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802f4479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 페이지에서 src 속성이 있는 태그에 연결된 내부 파일을 모두 내려받음.\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "downloadDirectory = 'downloaded'\n",
    "baseUrl = 'http://pythonscraping.com'\n",
    "\n",
    "def getAbsoluteURL(baseUrl, source):\n",
    "    if source.startswith('http://www.'):\n",
    "        url = 'http://{}'.format(source[11:])\n",
    "    elif source.startswith('http://'):\n",
    "        url = source\n",
    "    elif source.startswith('www.'):\n",
    "        url = source[4:]\n",
    "        url = 'http://{}'.format(source)\n",
    "    else:\n",
    "        url = '{}/{}'.format(baseUrl, source)\n",
    "    if baseUrl not in url:\n",
    "        return None\n",
    "    return url\n",
    "\n",
    "def getDownloadPath(baseUrl, absoluteUrl, downloadDirectory):\n",
    "    path = absoluteUrl.replace('www.', '')\n",
    "    path = path.replace(baseUrl, '')\n",
    "    path = downloadDirectory+path\n",
    "    directory = os.path.dirname(path)\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    return path\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "downloadList = bs.findAll(src=True)\n",
    "\n",
    "for download in downloadList:\n",
    "    fileUrl = getAbsoluteURL(baseUrl, download['src'])\n",
    "    if fileUrl is not None:\n",
    "        print(fileUrl)\n",
    "\n",
    "urlretrieve(fileUrl, getDownloadPath(baseUrl, fileUrl, downloadDirectory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920ceb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV(Comma-separated values): 쉼표로 구분된 값\n",
    "import csv\n",
    "\n",
    "csvFile = open('test.csv', 'w+') # test.csv가 없으면 생성함, 있으면 경고 없이 덮어씌움.\n",
    "try:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerow(('number', 'number plus 2', 'number times 2'))\n",
    "    for i in range(10):\n",
    "        writer.writerow( (i, i+2, i*2))\n",
    "finally:\n",
    "    csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72c6b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML 테이블을 가져와 CSV 파일 만들기\n",
    "import csv\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('http://en.wikipedia.org/wiki/Comparison_of_text_editors')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "# The main comparison table is currently the first table on the page\n",
    "# 비교 테이블은 현재 페이지의 첫 번째 테이블.\n",
    "table = bs.findAll('table',{'class':'wikitable'})[0]\n",
    "rows = table.findAll('tr')\n",
    "\n",
    "csvFile = open('editors.csv', 'wt+', encoding='utf-8')\n",
    "writer = csv.writer(csvFile)\n",
    "try:\n",
    "    for row in rows:\n",
    "        csvRow = []\n",
    "        for cell in row.findAll(['td', 'th']):\n",
    "            csvRow.append(cell.get_text())\n",
    "        writer.writerow(csvRow)\n",
    "finally:\n",
    "    csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f120b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MySQL과 연결\n",
    "# !pip install pymysql\n",
    "import pymysql\n",
    "\n",
    "# 연결 객체: 데이터베이스 연결, 정보 전송, 롤백, 커서 객체 생성\n",
    "conn = pymysql.connect(host='127.0.0.1',\n",
    "                       user='root', passwd='1234', db='mysql')\n",
    "# 커서 객체: 어떤 데이터베이스 사용 중인지 상태 정보 추적, 마지막 쿼리 결과도 저장함.\n",
    "cur = conn.cursor()\n",
    "cur.execute('USE scraping')\n",
    "cur.execute(\"SELECT * FROM pages WHERE id=1\")\n",
    "print(cur.fetchone()) # .fetchone(): 정보 접근\n",
    "cur.close()\n",
    "conn.close() # => 해야 connection leak(연결 누수) 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e375481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위키백과의 이것저것 데이터베이스에 저장\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import random\n",
    "import pymysql\n",
    "import re\n",
    "\n",
    "conn = pymysql.connect(host='127.0.0.1',\n",
    "                       user='root', passwd='1234', db='mysql', charset='utf8')\n",
    "cur = conn.cursor()\n",
    "cur.execute('USE scraping')\n",
    "\n",
    "random.seed(datetime.datetime.now().timestamp())\n",
    "\n",
    "def store(title, content):\n",
    "    cur.execute('INSERT INTO pages (title, content) VALUES (\"%s\", \"%s\")', (title, content))\n",
    "    cur.connection.commit()\n",
    "\n",
    "def getLinks(articleUrl):\n",
    "    html = urlopen('http://en.wikipedia.org'+articleUrl)\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    title = bs.find('h1').get_text()\n",
    "    content = bs.find('div', {'id':'mw-content-text'}).find('p').get_text()\n",
    "    store(title, content)\n",
    "    return bs.find('div', {'id':'bodyContent'}).findAll('a', href=re.compile('^(/wiki/)((?!:).)*$'))\n",
    "\n",
    "links = getLinks('/wiki/Kevin_Bacon')\n",
    "try:\n",
    "    while len(links) > 0:\n",
    "         newArticle = links[random.randint(0, len(links)-1)].attrs['href']\n",
    "         print(newArticle)\n",
    "         links = getLinks(newArticle)\n",
    "finally:\n",
    "    cur.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a1e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pymysql\n",
    "from random import shuffle\n",
    "\n",
    "conn = pymysql.connect(host='127.0.0.1',\n",
    "                       user='root', passwd='1234', db='mysql', charset='utf8')\n",
    "cur = conn.cursor()\n",
    "cur.execute('USE wikipedia')\n",
    "\n",
    "# 새 페이지 발견할 때마다 저장. 페이지 중복 저장X, +새 링크 생성시 사용할 pageId를 검색하는 역할.\n",
    "def insertPageIfNotExists(url):\n",
    "    cur.execute('SELECT * FROM pages WHERE url = %s', (url))\n",
    "    if cur.rowcount == 0:\n",
    "        cur.execute('INSERT INTO pages (url) VALUES (%s)', (url))\n",
    "        conn.commit()\n",
    "        return cur.lastrowid\n",
    "    else:\n",
    "        return cur.fetchone()[0]\n",
    "\n",
    "# 이론적으로 필요없는 경우 있음. 새 페이지에 방문해야하는지 결정하는데 사용.\n",
    "def loadPages():\n",
    "    cur.execute('SELECT * FROM pages')\n",
    "    pages = [row[1] for row in cur.fetchall()]\n",
    "    return pages\n",
    "\n",
    "# 데이터베이스에 링크를 기록. 다수 실행시도 데이터베이스의 무결성 보장.\n",
    "def insertLink(fromPageId, toPageId):\n",
    "    cur.execute('SELECT * FROM links WHERE fromPageId = %s AND toPageId = %s', \n",
    "                  (int(fromPageId), int(toPageId)))\n",
    "    if cur.rowcount == 0:\n",
    "        cur.execute('INSERT INTO links (fromPageId, toPageId) VALUES (%s, %s)', \n",
    "                    (int(fromPageId), int(toPageId)))\n",
    "        conn.commit()\n",
    "def pageHasLinks(pageId):\n",
    "    cur.execute('SELECT * FROM links WHERE fromPageId = %s', (int(pageId)))\n",
    "    rowcount = cur.rowcount\n",
    "    if rowcount == 0:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def getLinks(pageUrl, recursionLevel, pages):\n",
    "    if recursionLevel > 4:\n",
    "        return\n",
    "\n",
    "    pageId = insertPageIfNotExists(pageUrl)\n",
    "    html = urlopen('http://en.wikipedia.org{}'.format(pageUrl))\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    links = bs.findAll('a', href=re.compile('^(/wiki/)((?!:).)*$'))\n",
    "    links = [link.attrs['href'] for link in links]\n",
    "\n",
    "    for link in links:\n",
    "        linkId = insertPageIfNotExists(link)\n",
    "        insertLink(pageId, linkId)\n",
    "        if not pageHasLinks(linkId):\n",
    "            # 새 페이지를 만났으니 추가하고 링크를 검색.\n",
    "            print(\"PAGE HAS NO LINKS: {}\".format(link))\n",
    "            pages.append(link)\n",
    "            getLinks(link, recursionLevel+1, pages)\n",
    "        \n",
    "        \n",
    "getLinks('/wiki/Kevin_Bacon', 0, loadPages()) \n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdc3e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이메일 보내기\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "msg = MIMEText('The body of the email is here')\n",
    "\n",
    "msg['Subject'] = 'An Email Alert'\n",
    "msg['From'] = 'ryan@pythonscraping.com'\n",
    "msg['To'] = 'webmaster@pythonscraping.com'\n",
    "\n",
    "s = smtplib.SMTP('localhost')\n",
    "s.send_message(msg)\n",
    "s.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ace2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 시간에 한 번씩 https://isitchristmas.com 웹사이트를 체크. NO외 의 것이면 크리스마스 이메일이 올 것.\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import time\n",
    "\n",
    "def sendMail(subject, body):\n",
    "    msg = MIMEText(body)\n",
    "    msg['Subject'] = subject\n",
    "    msg['From'] ='christmas_alerts@pythonscraping.com'\n",
    "    msg['To'] = 'ryan@pythonscraping.com'\n",
    "\n",
    "    s = smtplib.SMTP('localhost')\n",
    "    s.send_message(msg)\n",
    "    s.quit()\n",
    "\n",
    "bs = BeautifulSoup(urlopen('https://isitchristmas.com/'), 'html.parser')\n",
    "while(bs.find('a', {'id':'answer'}).attrs['title'] == 'NO'):\n",
    "    print('It is not Christmas yet.')\n",
    "    time.sleep(3600)\n",
    "    bs = BeautifulSoup(urlopen('https://isitchristmas.com/'), 'html.parser')\n",
    "sendMail('It\\'s Christmas!', \n",
    "         'According to http://itischristmas.com, it is Christmas!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2324bead",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78981514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 인코딩 - txt\n",
    "from urllib.request import urlopen\n",
    "textPage = urlopen('http://www.pythonscraping.com/pages/warandpeace/chapter1.txt')\n",
    "print(textPage.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fd8f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 인코딩 - txt, incoding\n",
    "from urllib.request import urlopen\n",
    "# textPage = urlopen('http://www.pythonscraping.com/pages/warandpeace/chapter1-ru.txt')\n",
    "textPage = urlopen(\n",
    "             'http://www.pythonscraping.com/pages/warandpeace/chapter1-ru.txt')\n",
    "print(str(textPage.read(), 'utf-8')) # 키릴 문자로 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096ef104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "html = urlopen(\"http://en.wikipedia.org/wiki/Python_(programming_language)\")\n",
    "bs = BeautifulSoup(html, \"html.parser\")\n",
    "content = bs.find(\"div\", {\"id\":\"mw-content-text\"}).get_text()\n",
    "content = bytes(content, \"UTF-8\")\n",
    "content = content.decode(\"UTF-8\")\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d2e843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 인코딩 - csv(몬티 파이튼 앨범 목록)\n",
    "from urllib.request import urlopen\n",
    "from io import StringIO\n",
    "import csv\n",
    "\n",
    "data = urlopen('http://pythonscraping.com/files/MontyPythonAlbums.csv').read().decode('ascii', 'ignore')\n",
    "dataFile = StringIO(data)\n",
    "csvReader = csv.reader(dataFile) # iterable(순환체)를 반환함, 리스트 객체\n",
    "\n",
    "# for row in csvReader:\n",
    "#     print(row)\n",
    "#     print(\"The album \\\"\"+row[0]+\"\\\" was released in \"+str(row[1]))\n",
    "for row in csvReader:\n",
    "    print(\"The album \\\"\"+row[0]+\"\\\" was released in \"+str(row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DictReader: csvReader 보다 오래 걸리지만 첫 번째 행 무시나 처리 등을 간편하게 해줌.\n",
    "from urllib.request import urlopen\n",
    "from io import StringIO\n",
    "import csv\n",
    "\n",
    "data = urlopen(\"http://pythonscraping.com/files/MontyPythonAlbums.csv\").read().decode('ascii', 'ignore')\n",
    "dataFile = StringIO(data)\n",
    "dictReader = csv.DictReader(dataFile)\n",
    "\n",
    "print(dictReader.fieldnames)\n",
    "\n",
    "for row in dictReader:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06899a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 인코딩 - PDF, 로컬 파일 객체로 바꿔 문자열로 읽음.\n",
    "# !pip install pdfminer3k\n",
    "from urllib.request import urlopen\n",
    "from pdfminer.pdfinterp import PDFResourceManager, process_pdf\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from io import StringIO\n",
    "from io import open\n",
    "\n",
    "def readPDF(pdfFile):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, laparams=laparams)\n",
    "\n",
    "    process_pdf(rsrcmgr, device, pdfFile)\n",
    "    device.close()\n",
    "\n",
    "    content = retstr.getvalue()\n",
    "    retstr.close()\n",
    "    return content\n",
    "\n",
    "pdfFile = urlopen(\"http://pythonscraping.com/pages/warandpeace/chapter1.pdf\")\n",
    "outputString = readPDF(pdfFile)\n",
    "print(outputString)\n",
    "pdfFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c10957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 인코딩 - DOCX( XML 읽기 )\n",
    "# 원격 워드 문서를 바이너리 파일 객체로 읽고 압축을 풀어 XML을 읽는다.\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "wordFile = urlopen('http://pythonscraping.com/pages/AWordDocument.docx').read()\n",
    "wordFile = BytesIO(wordFile)\n",
    "document = ZipFile(wordFile)\n",
    "xml_content = document.read('word/document.xml')\n",
    "# print(xml_content.decode('utf-8')) # xml 읽기\n",
    "wordObj = BeautifulSoup(xml_content.decode('utf-8'), 'xml')\n",
    "textStrings = wordObj.find_all('w:t')\n",
    "\n",
    "for textElem in textStrings:\n",
    "    print(textElem.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71356be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beatuiful Soup의 내비게이션 기능을 활용\n",
    "textStrings = wordObj.find_all('w:t')\n",
    "\n",
    "for textElem in textStrings:\n",
    "    style = textElem.parent.parent.find('w:pStyle')\n",
    "    if style is not None and style['w:val'] == 'Title':\n",
    "        print('Title is: {}'.format(textElem.text))\n",
    "    else:\n",
    "        print(textElem.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a594296",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a343c2c6",
   "metadata": {},
   "source": [
    "**n-그램**: 텍스트나 연설에서 연속으로 나타난 단어 n개.\n",
    "- 자연어를 분석할 때 공통적으로 나타나는 n-그램\n",
    "- 자주 함께 쓰이는 단어 집합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3420543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확한 형태를 갖춘 n-그램을 찾기, 위키백과 항목에서 찾은 2-그램 목록 반환\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "\n",
    "# 문장을 단어로 분할, 구두점과 공백 제거, 한 글자로 이루어진 단어(I, a 제외) 제거\n",
    "def cleanSentence(sentence):\n",
    "    sentence = sentence.split(' ')\n",
    "    sentence = [word.strip(string.punctuation+string.whitespace) for word in sentence]\n",
    "    sentence = [word for word in sentence if len(word) > 1 or (word.lower() == 'a' or word.lower() == 'i')]\n",
    "    return sentence\n",
    "\n",
    "# 줄바꿈 문자와 인용 기호 제거, 마침표 뒤 공백 기준으로 텍스트를 '문장'으로 분할\n",
    "def cleanInput(content):\n",
    "    content = content.upper()\n",
    "    content = re.sub('\\n|[[\\d+\\]]', ' ', content)\n",
    "    content = bytes(content, \"UTF-8\")\n",
    "    content = content.decode(\"ascii\", \"ignore\")\n",
    "    sentences = content.split('. ')\n",
    "    return [cleanSentence(sentence) for sentence in sentences]\n",
    "\n",
    "# n-그램을 만드는 핵심 기능, getNgrams에서 매 문장마다 호출함(문장에 걸치는 n-그램 형성 제어).\n",
    "def getNgramsFromSentence(content, n):\n",
    "    output = []\n",
    "    for i in range(len(content)-n+1):\n",
    "        output.append(content[i:i+n])\n",
    "    return output\n",
    "\n",
    "# 기본적인 진입점\n",
    "def getNgrams(content, n):\n",
    "    content = cleanInput(content)\n",
    "    ngrams = []\n",
    "    for sentence in content:\n",
    "        ngrams.extend(getNgramsFromSentence(sentence, n))\n",
    "    return(ngrams)\n",
    "        \n",
    "html = urlopen('http://en.wikipedia.org/wiki/Python_(programming_language)')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "content = bs.find('div', {'id':'mw-content-text'}).get_text()\n",
    "ngrams = getNgrams(content, 2)\n",
    "print(ngrams)\n",
    "print('2-grams count is: '+str(len(ngrams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f08bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen('http://en.wikipedia.org/wiki/Python_(programming_language)')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "content = bs.find('div', {'id':'mw-content-text'}).get_text()\n",
    "print(len(getNgrams(content, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4703285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복 n-그램 제거\n",
    "from collections import Counter\n",
    "\n",
    "def getNgrams(content, n):\n",
    "    content = cleanInput(content)\n",
    "    ngrams = Counter()\n",
    "    ngrams_list = []\n",
    "    for sentence in content:\n",
    "        newNgrams = [' '.join(ngram) for ngram in getNgramsFromSentence(sentence, n)]\n",
    "        ngrams_list.extend(newNgrams)\n",
    "        ngrams.update(newNgrams)\n",
    "    return(ngrams)\n",
    "\n",
    "print(getNgrams(content, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce9c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 요약: n-그램을 찾고 정렬\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "def cleanSentence(sentence):\n",
    "    sentence = sentence.split(' ')\n",
    "    sentence = [word.strip(string.punctuation+string.whitespace) for word in sentence]\n",
    "    sentence = [word for word in sentence if len(word) > 1 or (word.lower() == 'a' or word.lower() == 'i')]\n",
    "    return sentence\n",
    "\n",
    "def cleanInput(content):\n",
    "    content = content.upper()\n",
    "    content = re.sub('\\n', ' ', content)\n",
    "    content = bytes(content, 'UTF-8')\n",
    "    content = content.decode('ascii', 'ignore')\n",
    "    sentences = content.split('. ')\n",
    "    return [cleanSentence(sentence) for sentence in sentences]\n",
    "\n",
    "def getNgramsFromSentence(content, n):\n",
    "    output = []\n",
    "    for i in range(len(content)-n+1):\n",
    "        output.append(content[i:i+n])\n",
    "    return output\n",
    "\n",
    "def getNgrams(content, n):\n",
    "    content = cleanInput(content)\n",
    "    ngrams = Counter()\n",
    "    ngrams_list = []\n",
    "    for sentence in content:\n",
    "        newNgrams = [' '.join(ngram) for ngram in getNgramsFromSentence(sentence, n)]\n",
    "        ngrams_list.extend(newNgrams)\n",
    "        ngrams.update(newNgrams)\n",
    "    return(ngrams)\n",
    "\n",
    "\n",
    "content = str(\n",
    "      urlopen('http://pythonscraping.com/files/inaugurationSpeech.txt').read(),\n",
    "              'utf-8')\n",
    "ngrams = getNgrams(content, 3)\n",
    "print(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fef2a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요없는 단어들 필터링.\n",
    "# 현대 미국 영어 자료 (http://corpus.byu.edu/coca/)\n",
    "def isCommon(ngram):\n",
    "    commonWords = ['THE', 'BE', 'AND', 'OF', 'A', 'IN', 'TO', 'HAVE', 'IT', 'I', 'THAT', 'FOR', 'YOU', 'HE', 'WITH', 'ON', 'DO', \n",
    "                   'SAY', 'THIS', 'THEY', 'IS', 'AN', 'AT', 'BUT', 'WE', 'HIS', 'FROM', 'THAT', 'NOT', 'BY', 'SHE', 'OR', 'AS', \n",
    "                   'WHAT', 'GO', 'THEIR', 'CAN', 'WHO', 'GET', 'IF', 'WOULD', 'HER', 'ALL', 'MY', 'MAKE', 'ABOUT', 'KNOW', 'WILL', \n",
    "                   'AS', 'UP', 'ONE', 'TIME', 'HAS', 'BEEN', 'THERE', 'YEAR', 'SO', 'THINK', 'WHEN', 'WHICH', 'THEM', 'SOME', 'ME', \n",
    "                   'PEOPLE', 'TAKE', 'OUT', 'INTO', 'JUST', 'SEE', 'HIM', 'YOUR', 'COME', 'COULD', 'NOW', 'THAN', 'LIKE', 'OTHER', \n",
    "                   'HOW', 'THEN', 'ITS', 'OUR', 'TWO', 'MORE', 'THESE', 'WANT', 'WAY', 'LOOK', 'FIRST', 'ALSO', 'NEW', 'BECAUSE', \n",
    "                   'DAY', 'MORE', 'USE', 'NO', 'MAN', 'FIND', 'HERE', 'THING', 'GIVE', 'MANY', 'WELL']\n",
    "    for word in ngram:\n",
    "        if word in commonWords:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def getNgramsFromSentence(content, n):\n",
    "    output = []\n",
    "    for i in range(len(content)-n+1):\n",
    "        if not isCommon(content[i:i+n]):\n",
    "            output.append(content[i:i+n])\n",
    "    return output\n",
    "\n",
    "ngrams = getNgrams(content, 3)\n",
    "print(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8580d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 텍스트(content)에서 각 문장을 대문자로 변환, \n",
    "# n-gram이 해당 문장에 포함되어 있는지 확인, \n",
    "# 만약 n-gram이 포함되어 있다면 해당 문장을 반환하고, 그렇지 않으면 빈 문자열을 반환\n",
    "def getFirstSentenceContaining(ngram, content):\n",
    "    #print(ngram)\n",
    "    sentences = content.upper().split(\". \")\n",
    "    for sentence in sentences: \n",
    "        if ngram in sentence:\n",
    "            return sentence+'\\n'\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "print(getFirstSentenceContaining('EXCLUSIVE METALLIC CURRENCY', content))\n",
    "print(getFirstSentenceContaining('EXECUTIVE DEPARTMENT', content))\n",
    "print(getFirstSentenceContaining('GENERAL GOVERNMENT', content))\n",
    "print(getFirstSentenceContaining('CALLED UPON', content))\n",
    "print(getFirstSentenceContaining('CHIEF MAGISTRATE', content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd21745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마르코프 모델: 어떤 특정 사건이 다른 특정 사건에 뒤이어, 일정 확률로 일어나는 대규모 무작위 분포를 분석할 때 쓰임.\n",
    "from urllib.request import urlopen\n",
    "from random import randint\n",
    "\n",
    "def wordListSum(wordList):\n",
    "    sum = 0\n",
    "    for word, value in wordList.items():\n",
    "        sum += value\n",
    "    return sum\n",
    "\n",
    "def retrieveRandomWord(wordList):\n",
    "    randIndex = randint(1, wordListSum(wordList))\n",
    "    for word, value in wordList.items():\n",
    "        randIndex -= value\n",
    "        if randIndex <= 0:\n",
    "            return word\n",
    "\n",
    "def buildWordDict(text):\n",
    "    # Remove newlines and quotes\n",
    "    # 줄바꿈 문자와 따옴표를 제거.\n",
    "    text = text.replace('\\n', ' ');\n",
    "    text = text.replace('\"', '');\n",
    "\n",
    "    # Make sure punctuation marks are treated as their own \"words,\"\n",
    "    # so that they will be included in the Markov chain\n",
    "    # 구두점 역시 단어로 취급해서 마르코프 체인에 들어가도록 함.\n",
    "    punctuation = [',','.',';',':']\n",
    "    for symbol in punctuation:\n",
    "        text = text.replace(symbol, ' {} '.format(symbol));\n",
    "\n",
    "    words = text.split(' ')\n",
    "    # Filter out empty words\n",
    "    # 빈 단어를 제거.\n",
    "    words = [word for word in words if word != '']\n",
    "\n",
    "    wordDict = {}\n",
    "    for i in range(1, len(words)):\n",
    "        if words[i-1] not in wordDict:\n",
    "                # Create a new dictionary for this word\n",
    "                # 이 단어에 필요한 새 딕셔너리 생성.\n",
    "            wordDict[words[i-1]] = {}\n",
    "        if words[i] not in wordDict[words[i-1]]:\n",
    "            wordDict[words[i-1]][words[i]] = 0\n",
    "        wordDict[words[i-1]][words[i]] += 1\n",
    "    return wordDict\n",
    "\n",
    "text = str(urlopen('http://pythonscraping.com/files/inaugurationSpeech.txt')\n",
    "          .read(), 'utf-8')\n",
    "wordDict = buildWordDict(text)\n",
    "\n",
    "#Generate a Markov chain of length 100\n",
    "# 길이가 100인(자유 설정) 마르코프 체인 생성.\n",
    "length = 100\n",
    "chain = ['I']\n",
    "for i in range(0, length):\n",
    "    newWord = retrieveRandomWord(wordDict[chain[-1]])\n",
    "    chain.append(newWord)\n",
    "\n",
    "print(' '.join(chain))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9e203e",
   "metadata": {},
   "source": [
    "**너비 우선 탐색**; 방향성 그래프(시작 페이지에서 목표 페이지에 도달하는 링크 체인을 찾는 문제)에서 가장 짧은 경로를 찾을 때 가장 좋은 방법<br>\n",
    "우선 시작 페이지에서 출발하는 링크를 모두 검색. 검색 링크에 목표 페이지가 없으면 2단계 링크, <br>\n",
    "시작 페이지에서 링크된 페이지에서 다시 링크된 페이지를 찾음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29084f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "conn = pymysql.connect(host='127.0.0.1', user='root', passwd='1234', db='mysql', charset='utf8')\n",
    "cur = conn.cursor()\n",
    "cur.execute('USE wikipedia')\n",
    "\n",
    "# 페이지 ID를 받아서 데이터베이스에서 URL을 가져오는 보조 함수.\n",
    "def getUrl(pageId):\n",
    "    cur.execute('SELECT url FROM pages WHERE id = %s', (int(pageId)))\n",
    "    return cur.fetchone()[0]\n",
    "\n",
    "# 현재 페이지를 나타내는 정수(fromPageId)를 받아서, 현재 페이지에서 링크한 ID를 전부 가져오는 보조 함수.\n",
    "def getLinks(fromPageId):\n",
    "    cur.execute('SELECT toPageId FROM links WHERE fromPageId = %s', (int(fromPageId)))\n",
    "    if cur.rowcount == 0:\n",
    "        return []\n",
    "    return [x[0] for x in cur.fetchall()]\n",
    "\n",
    "# 메인 함수. 검색 페이지에서 출발해 대상 페이지까지 도달하는 경로를 만날 때까지, \n",
    "# 재귀적으로 동작하면서 가능한 경로를 전부 리스트에 담는다.\n",
    "def searchBreadth(targetPageId, paths=[[1]]):\n",
    "    newPaths = []\n",
    "    for path in paths:\n",
    "        links = getLinks(path[-1])\n",
    "        for link in links:\n",
    "            if link == targetPageId:\n",
    "                return path + [link]\n",
    "            else:\n",
    "                newPaths.append(path+[link])\n",
    "    return searchBreadth(targetPageId, newPaths)\n",
    "                \n",
    "nodes = getLinks(1)\n",
    "targetPageId = 500\n",
    "pageIds = searchBreadth(targetPageId)\n",
    "for pageId in pageIds:\n",
    "    print(getUrl(pageId))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ba5f91",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbefcaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "params = {'firstname': 'Jieun', 'lastname': 'Park'}\n",
    "r = requests.post(\"http://pythonscraping.com/pages/processing.php\", data=params)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6aed95",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222e24a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3481b430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON 파싱\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "\n",
    "def getCountry(ipAddress):\n",
    "    url = 'http://api.ipstack.com/' + ipAddress\n",
    "    url += '?access_key=ACCESS_KEY&amp;format=1'\n",
    "    response = urlopen(url).read().decode('utf-8')\n",
    "    responseJson = json.loads(response)\n",
    "    return responseJson.get('country_code')\n",
    "\n",
    "print(getCountry('50.78.253.58')) # IP주소가 50.78.253.58인 국가 코드(US) 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21e8fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위키백과를 크롤링해 개정 내역 페이지를 찾아보고 그 페이지에서 IP 주소를 찾아내는 스크립트\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import datetime\n",
    "import random\n",
    "import re\n",
    "\n",
    "random.seed(datetime.datetime.now().timestamp())\n",
    "def getLinks(articleUrl):\n",
    "    html = urlopen('http://en.wikipedia.org{}'.format(articleUrl))\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    return bs.find('div', {'id':'bodyContent'}).findAll('a', \n",
    "        href=re.compile('^(/wiki/)((?!:).)*$'))\n",
    "\n",
    "def getHistoryIPs(pageUrl):\n",
    "    #Format of revision history pages is: 개정 히스토리 페이지 형식은 다음과 같음: \n",
    "    #http://en.wikipedia.org/w/index.php?title=Title_in_URL&action=history\n",
    "    pageUrl = pageUrl.replace('/wiki/', '')\n",
    "    historyUrl = 'http://en.wikipedia.org/w/index.php?title={}&action=history'.format(pageUrl)\n",
    "    print('history url is: {}'.format(historyUrl))\n",
    "    html = urlopen(historyUrl)\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    #finds only the links with class \"mw-anonuserlink\" which has IP addresses \n",
    "    #instead of usernames\n",
    "    # 클래스가 \"mw-anonuserlink\"인, 사용자 이름이 아니라 IP 주소가 들어있는 링크만 찾음. \n",
    "    ipAddresses = bs.findAll('a', {'class':'mw-anonuserlink'})\n",
    "    addressList = set()\n",
    "    for ipAddress in ipAddresses:\n",
    "        addressList.add(ipAddress.get_text())\n",
    "    return addressList\n",
    "\n",
    "links = getLinks('/wiki/Python_(programming_language)')\n",
    "\n",
    "while(len(links) > 0):\n",
    "    for link in links:\n",
    "        print('-'*20) \n",
    "        historyIPs = getHistoryIPs(link.attrs['href'])\n",
    "        for historyIP in historyIPs:\n",
    "            print(historyIP)\n",
    "\n",
    "    newLink = links[random.randint(0, len(links)-1)].attrs['href']\n",
    "    links = getLinks(newLink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a32fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCountry(ipAddress):\n",
    "    try:\n",
    "        response = urlopen(\n",
    "            'http://freegeoip.net/json/{}'.format(ipAddress)).read().decode('utf-8')\n",
    "    except HTTPError:\n",
    "        return None\n",
    "    responseJson = json.loads(response)\n",
    "    return responseJson.get('country_code')\n",
    "    \n",
    "links = getLinks('/wiki/Python_(programming_language)')\n",
    "\n",
    "while(len(links) > 0):\n",
    "    for link in links:\n",
    "        print('-'*20) \n",
    "        historyIPs = getHistoryIPs(link.attrs[\"href\"])\n",
    "        for historyIP in historyIPs:\n",
    "            country = getCountry(historyIP)\n",
    "            if country is not None:\n",
    "                print('{} is from {}'.format(historyIP, country))\n",
    "\n",
    "    newLink = links[random.randint(0, len(links)-1)].attrs['href']\n",
    "    links = getLinks(newLink)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5dff25",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d896983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unittest: 단위 테스트 모듈\n",
    "import unittest\n",
    "\n",
    "class TestAddition(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        print('Setting up the test')\n",
    "\n",
    "    def tearDown(self):\n",
    "        print('Tearing down the test')\n",
    "\n",
    "    def test_twoPlusTwo(self):\n",
    "        total = 2+2\n",
    "        print(total)\n",
    "        self.assertEqual(4, total);\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ad4377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웹사이트 프론트엔드 테스트\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import unittest\n",
    "\n",
    "class TestWikipedia(unittest.TestCase):\n",
    "    bs = None\n",
    "    def setUpClass():\n",
    "        url = 'http://en.wikipedia.org/wiki/Monty_Python'\n",
    "        TestWikipedia.bs = BeautifulSoup(urlopen(url), 'html.parser')\n",
    "\n",
    "    def test_titleText(self):\n",
    "        pageTitle = TestWikipedia.bs.find('h1').get_text() # 페이지 타이틀이 Monty_Python인가 확인\n",
    "        self.assertEqual('Monty Python', pageTitle);\n",
    "\n",
    "    def test_contentExists(self):\n",
    "        content = TestWikipedia.bs.find('div',{'id':'mw-content-text'})  # 페이지에 콘텐츠 div가 있는지 확인\n",
    "        self.assertIsNotNone(content)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)\n",
    "    %reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b2fe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웹사이트 프론트엔드 테스트, 반복 실행\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import unittest\n",
    "import re\n",
    "import random\n",
    "from urllib.parse import unquote\n",
    "\n",
    "class TestWikipedia(unittest.TestCase):\n",
    "\n",
    "    def test_PageProperties(self):\n",
    "        self.url = 'http://en.wikipedia.org/wiki/Monty_Python'\n",
    "        #Test the first 10 pages we encounter\n",
    "        # 만드는 순서에 따라 페이지 10개를 테스트함.\n",
    "        for i in range(1, 10):\n",
    "            self.bs = BeautifulSoup(urlopen(self.url), 'html.parser')\n",
    "            titles = self.titleMatchesURL()\n",
    "            self.assertEqual(titles[0], titles[1])\n",
    "            self.assertTrue(self.contentExists())\n",
    "            self.url = self.getNextLink()\n",
    "        print('Done!')\n",
    "\n",
    "    def titleMatchesURL(self):\n",
    "        pageTitle = self.bs.find('h1').get_text()\n",
    "        urlTitle = self.url[(self.url.index('/wiki/')+6):]\n",
    "        urlTitle = urlTitle.replace('_', ' ')\n",
    "        urlTitle = unquote(urlTitle)\n",
    "        return [pageTitle.lower(), urlTitle.lower()]\n",
    "\n",
    "    def contentExists(self):\n",
    "        content = self.bs.find('div',{'id':'mw-content-text'})\n",
    "        if content is not None:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def getNextLink(self):\n",
    "        # Returns random link on page, using technique from Chapter 3\n",
    "        links = self.bs.find('div', {'id':'bodyContent'}).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$'))\n",
    "        randomLink = random.SystemRandom().choice(links)\n",
    "        return 'https://wikipedia.org{}'.format(randomLink.attrs['href'])\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)\n",
    "    %reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29a744c",
   "metadata": {},
   "source": [
    "### 병렬 웹 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016cbb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 멀티 스레드 크롤링\n",
    "import _thread\n",
    "import time\n",
    "\n",
    "def print_time(threadName, delay, iterations):\n",
    "    start = int(time.time())\n",
    "    for i in range(0,iterations):\n",
    "        time.sleep(delay)\n",
    "        seconds_elapsed = str(int(time.time()) - start)\n",
    "        print (threadName if threadName else seconds_elapsed)\n",
    "\n",
    "try:\n",
    "    _thread.start_new_thread(print_time, (None, 1, 100))\n",
    "    _thread.start_new_thread(print_time, (\"Fizz\", 3, 33))\n",
    "    _thread.start_new_thread(print_time, (\"Buzz\", 5, 20))\n",
    "except:\n",
    "    print (\"Error: unable to start thread\")\n",
    "\n",
    "while 1:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3071ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n",
    "\n",
    "import _thread\n",
    "import time\n",
    "\n",
    "visited = [] # 재방문 제어\n",
    "def getLinks(thread_name, bsObj):\n",
    "    print('Getting links in {}'.format(thread_name))\n",
    "    links = bsObj.find('div', {'id':'bodyContent'}).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$'))\n",
    "    return [link for link in links if link not in visited]\n",
    "\n",
    "# Define a function for the thread\n",
    "# 스레드에서 실행할 함수\n",
    "def scrape_article(thread_name, path):\n",
    "    visited.append(path)\n",
    "    html = urlopen('http://en.wikipedia.org{}'.format(path))\n",
    "    time.sleep(5)\n",
    "    bsObj = BeautifulSoup(html, 'html.parser')\n",
    "    title = bsObj.find('h1').get_text()\n",
    "    print('Scraping {} in thread {}'.format(title, thread_name))\n",
    "    links = getLinks(thread_name, bsObj)\n",
    "    if len(links) > 0:\n",
    "        newArticle = links[random.randint(0, len(links)-1)].attrs['href']\n",
    "        print(newArticle)\n",
    "        scrape_article(thread_name, newArticle)\n",
    "\n",
    "\n",
    "# Create two threads as follows\n",
    "# 스레드 두 개를 만듦\n",
    "try:\n",
    "   _thread.start_new_thread(scrape_article, ('Thread 1', '/wiki/Kevin_Bacon',))\n",
    "   _thread.start_new_thread(scrape_article, ('Thread 2', '/wiki/Monty_Python',))\n",
    "except:\n",
    "   print ('Error: unable to start threads')\n",
    "\n",
    "while 1:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac22975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# queue(큐): 선입선출 or 후입선출 방식으로 작동하는 리스트 비슷 객체.\n",
    "# queue.put(): 어떤 스레드로부터 메시지를 받음, queue.get(): 호출하는 어떤 스레드에든 메시지 전달.\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n",
    "import _thread\n",
    "from queue import Queue\n",
    "import time\n",
    "import pymysql\n",
    "\n",
    "\n",
    "def storage(queue):\n",
    "    conn = pymysql.connect(host='127.0.0.1', unix_socket='/tmp/mysql.sock', user='root', passwd='', db='mysql', charset='utf8')\n",
    "    cur = conn.cursor()\n",
    "    cur.execute('USE wiki_threads')\n",
    "    while 1:\n",
    "        if not queue.empty():\n",
    "            article = queue.get()\n",
    "            cur.execute('SELECT * FROM pages WHERE path = %s', (article[\"path\"]))\n",
    "            if cur.rowcount == 0:\n",
    "                print(\"Storing article {}\".format(article[\"title\"]))\n",
    "                cur.execute('INSERT INTO pages (title, path) VALUES (%s, %s)', (article[\"title\"], article[\"path\"]))\n",
    "                conn.commit()\n",
    "            else:\n",
    "                print(\"Article already exists: {}\".format(article['title']))\n",
    "\n",
    "visited = []\n",
    "def getLinks(thread_name, bsObj):\n",
    "    print('Getting links in {}'.format(thread_name))\n",
    "    links = bsObj.find('div', {'id':'bodyContent'}).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$'))\n",
    "    return [link for link in links if link not in visited]\n",
    "\n",
    "def scrape_article(thread_name, path, queue):\n",
    "    visited.append(path)\n",
    "    html = urlopen('http://en.wikipedia.org{}'.format(path))\n",
    "    time.sleep(5)\n",
    "    bsObj = BeautifulSoup(html, 'html.parser')\n",
    "    title = bsObj.find('h1').get_text()\n",
    "    print('Added {} for storage in thread {}'.format(title, thread_name))\n",
    "    queue.put({\"title\":title, \"path\":path})\n",
    "    links = getLinks(thread_name, bsObj)\n",
    "    if len(links) > 0:\n",
    "        newArticle = links[random.randint(0, len(links)-1)].attrs['href']\n",
    "        scrape_article(thread_name, newArticle, queue)\n",
    "\n",
    "queue = Queue()\n",
    "try:\n",
    "   _thread.start_new_thread(scrape_article, ('Thread 1', '/wiki/Kevin_Bacon', queue,))\n",
    "   _thread.start_new_thread(scrape_article, ('Thread 2', '/wiki/Monty_Python', queue,))\n",
    "   _thread.start_new_thread(storage, (queue,))\n",
    "except:\n",
    "   print ('Error: unable to start threads')\n",
    "\n",
    "while 1:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e2d782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threading: _thread의 모든 기능을 노출하면서 더 깨끗한 상위 인터페이스\n",
    "import threading\n",
    "import time\n",
    "\n",
    "def print_time(threadName, delay, iterations):\n",
    "    start = int(time.time())\n",
    "    for i in range(0,iterations):\n",
    "        time.sleep(delay)\n",
    "        seconds_elapsed = str(int(time.time()) - start)\n",
    "        print ('{} {}'.format(seconds_elapsed, threadName))\n",
    "\n",
    "t = threading.Thread(target=print_time, args=('Fizz', 3, 33)).start()\n",
    "t = threading.Thread(target=print_time, args=('Buzz', 5, 20)).start()\n",
    "t = threading.Thread(target=print_time, args=('Counter', 1, 100)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1451c229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "def crawler(url):\n",
    "    data = threading.local() # => 로컬 스레드 데이터 쉽게 생성: \n",
    "    # 여러 스레드가 서로 다른 사이트를 스크랩하면서 각각 방문한 페이지 리스트를 관리할 때 유용\n",
    "    data.visited = []\n",
    "    # 크롤링 코드\n",
    "threading.Thread(target=crawler, args=('http://brookings.edu')).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a38ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threading.Thread객체 확장해 모니터링에 쓸 메서드 추가\n",
    "import threading\n",
    "import time\n",
    "\n",
    "class Crawler(threading.Thread):\n",
    "    def __init__(self):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.done = False\n",
    "        \n",
    "    # 크롤링 완료 확인\n",
    "    def isDone(self):\n",
    "        return self.done\n",
    "\n",
    "    def run(self):\n",
    "        time.sleep(5)\n",
    "        self.done = True\n",
    "        raise Exception('Something bad happened!')\n",
    "\n",
    "t = Crawler()\n",
    "t.start()\n",
    "\n",
    "while True:\n",
    "    time.sleep(1)\n",
    "    if t.isDone():\n",
    "        print('Done')\n",
    "        break\n",
    "    if not t.isAlive():\n",
    "        t = Crawler()\n",
    "        t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76a0ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 멀티프로세스 크롤링\n",
    "from multiprocessing import Process\n",
    "import time\n",
    "\n",
    "def print_time(threadName, delay, iterations):\n",
    "    start = int(time.time())\n",
    "    for i in range(0,iterations):\n",
    "        time.sleep(delay)\n",
    "        seconds_elapsed = str(int(time.time()) - start)\n",
    "        print (threadName if threadName else seconds_elapsed)\n",
    "\n",
    "\n",
    "processes = []\n",
    "processes.append(Process(target=print_time, args=(None, 1, 100)))\n",
    "processes.append(Process(target=print_time, args=(\"Fizz\", 3, 33)))\n",
    "processes.append(Process(target=print_time, args=(\"Buzz\", 5, 20)))\n",
    "\n",
    "for p in processes:\n",
    "    p.start()\n",
    "\n",
    "for p in processes:\n",
    "    p.join()\n",
    "    \n",
    "print(\"Program complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d1993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n",
    "\n",
    "from multiprocessing import Process, Queue\n",
    "import os\n",
    "import time\n",
    "import Thread\n",
    "\n",
    "def getLinks(bsObj, queue):\n",
    "    print('Getting links in {}'.format(os.getpid()))\n",
    "    links = bsObj.find('div', {'id':'bodyContent'}).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$'))\n",
    "    return [link for link in links if link not in queue.get()]\n",
    "\n",
    "def scrape_article(path, queue):\n",
    "    queue.get().append()\n",
    "    print(\"Process {} list is now: {}\".format(os.getpid(), visited))\n",
    "    html = urlopen('http://en.wikipedia.org{}'.format(path))\n",
    "    time.sleep(5)\n",
    "    bsObj = BeautifulSoup(html, 'html.parser')\n",
    "    title = bsObj.find('h1').get_text()\n",
    "    print('Scraping {} in process {}'.format(title, os.getpid()))\n",
    "    links = getLinks(bsObj)\n",
    "    if len(links) > 0:\n",
    "        newArticle = links[random.randint(0, len(links)-1)].attrs['href']\n",
    "        print(newArticle)\n",
    "        scrape_article(newArticle)\n",
    "\n",
    "processes = []\n",
    "queue = Queue()\n",
    "processes.append(Process(target=scrape_article, args=('/wiki/Kevin_Bacon', queue,)))\n",
    "processes.append(Process(target=scrape_article, args=('/wiki/Monty_Python', queue,)))\n",
    "\n",
    "for p in processes:\n",
    "    p.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dd2443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n",
    "from multiprocessing import Process, Queue\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "def task_delegator(taskQueue, foundUrlsQueue):\n",
    "    #Initialize with a task for each process\n",
    "    # 각 프로세스에서 처리할 작업을 초기화.\n",
    "    visited = ['/wiki/Kevin_Bacon', '/wiki/Monty_Python']\n",
    "    taskQueue.put('/wiki/Kevin_Bacon')\n",
    "    taskQueue.put('/wiki/Monty_Python')\n",
    "\n",
    "    while 1:\n",
    "        #Check to see if there are new links in the foundUrlsQueue for processing\n",
    "        # urlsQueue에 처리할 새 링크 있는지 확인\n",
    "        if not foundUrlsQueue.empty():\n",
    "            links = [link for link in foundUrlsQueue.get() if link not in visited]\n",
    "            for link in links:\n",
    "                #Add new link to the taskQueue\n",
    "                # 새 링크를 taskQueue에 추가\n",
    "                taskQueue.put(link)\n",
    "                #Add new link to the visited list\n",
    "                visited.append(link)\n",
    "\n",
    "def get_links(bsObj):\n",
    "    links = bsObj.find('div', {'id':'bodyContent'}).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$'))\n",
    "    return [link.attrs['href'] for link in links]\n",
    "\n",
    "def scrape_article(taskQueue, foundUrlsQueue):\n",
    "    while 1:\n",
    "        while taskQueue.empty():\n",
    "            #Sleep 100 ms while waiting for the task queue \n",
    "            #This should be rare\n",
    "            # 작업 큐가 비어있으면 0.1초 대기(드물게 발생)\n",
    "            time.sleep(.1)\n",
    "        path = taskQueue.get()\n",
    "        html = urlopen('http://en.wikipedia.org{}'.format(path))\n",
    "        time.sleep(5)\n",
    "        bsObj = BeautifulSoup(html, 'html.parser')\n",
    "        title = bsObj.find('h1').get_text()\n",
    "        print('Scraping {} in process {}'.format(title, os.getpid()))\n",
    "        links = get_links(bsObj)\n",
    "        #Send these to the delegator for processing\n",
    "        # 찾아낸 링크를 위임자에 보내 처리하게함.\n",
    "        foundUrlsQueue.put(links)\n",
    "\n",
    "\n",
    "processes = []\n",
    "taskQueue = Queue()\n",
    "foundUrlsQueue = Queue()\n",
    "processes.append(Process(target=task_delegator, args=(taskQueue, foundUrlsQueue,)))\n",
    "processes.append(Process(target=scrape_article, args=(taskQueue, foundUrlsQueue,)))\n",
    "processes.append(Process(target=scrape_article, args=(taskQueue, foundUrlsQueue,)))\n",
    "\n",
    "for p in processes:\n",
    "    p.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
